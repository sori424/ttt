from tqdm import tqdm
from collections import Counter
import pandas as pd
import torch
import evaluate
from collections import defaultdict
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import sys


### Evaluation utils for Fantom

def compute_f1(ground_truth, model_response):
    """
    Compute the F1 score between the ground truth and model response.

    Args:
        ground_truth (str): The ground truth text.
        model_response (str): The model's response text.

    Returns:
        float: The F1 score.
    """
    ground_truth = ground_truth.split()
    model_response = model_response.split()
    common = Counter(ground_truth) & Counter(model_response)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(model_response)
    recall = 1.0 * num_same / len(ground_truth)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def evaluate_belief_q(qa, model_response, metric='cosine'):
    """
    Evaluate the belief question by comparing the model's response with the correct answer and wrong answer.

    Args:
        qa (dict): A dictionary containing the question and answers.
        model_response (str): The model's response to the question.
        metric (str, optional): The similarity metric to use for comparison. Defaults to 'cosine'.

    Returns:
        tuple: A tuple containing a boolean value indicating if the model's response matches the correct answer,
                and the lexical overlap score between the model's response and the corresponding answer.
    """
    embedder = SentenceTransformer('sentence-transformers/all-roberta-large-v1')
    wrong_tom_view = qa['wrong_answer']
    if metric == "cosine":
        wrong_tom_view_emb = embedder.encode(wrong_tom_view)
        personx_view_emb = embedder.encode(qa['correct_answer'])
        model_response_emb = embedder.encode(model_response)
        similarity_wrong_tom_view = cosine_similarity(model_response_emb.reshape(1, -1), wrong_tom_view_emb.reshape(1, -1))[0][0]
        similarity_personx_view = cosine_similarity(model_response_emb.reshape(1, -1), personx_view_emb.reshape(1, -1))[0][0]
    else:
        raise NotImplementedError

    if similarity_wrong_tom_view >= similarity_personx_view:
        wrong_view_lexical_overlap = compute_f1(wrong_tom_view, model_response)
        return False, wrong_view_lexical_overlap
    else:
        personx_view_lexical_overlap = compute_f1(qa['correct_answer'], model_response)
        return True, personx_view_lexical_overlap

def evaluate_mc_belief_q(qa, model_response):
    """
    Evaluate the multiple-choice version belief question.

    Args:
        qa (dict): The question and answer information.
        model_response (str): The model's response to the question.

    Returns:
        bool: True if the model's response matches the correct answer, False otherwise.
    """
    int_to_alphabet = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}
    answer = int_to_alphabet[qa['correct_answer']]
    response = model_response.lower()

    if response.startswith("(" + answer + ")") or response.startswith(answer + ")") or response.startswith(answer + ".") or response.startswith(answer + ":") or response.startswith(answer + ",") or "({})".format(answer) in response or answer == response: # a) or a. or a or (a)
        return True
    else:
        return False

def evaluate_list_q(qa, model_response):
    """
    Check whether all the characters in the correct answer are in the model's response
    and none of the characters in the wrong answer are in the model's response

    Args:
        qa (dict): A dictionary containing the question and answer information.
        model_response (str): The response generated by the model.

    Returns:
        tuple: A tuple containing three values:
            - A boolean indicating whether the model's response satisfies the evaluation criteria.
            - A boolean indicating whether any aware characters were excluded from the model's response.
            - A boolean indicating whether any unaware characters were included in the model's response.
    """
    excluded_aware_character = False
    included_unaware_character = False
    for character in qa['correct_answer']:
        if character.lower() not in model_response.lower():
            excluded_aware_character = True
            break

    for character in qa['wrong_answer']:
        if character.lower() in model_response.lower():
            included_unaware_character = True
            break

    return not(excluded_aware_character or included_unaware_character), excluded_aware_character, included_unaware_character

def map_binary_answer_to_int(model_response):
    """
    Maps a binary answer to an integer value.

    Args:
        model_response (str): The model's response.

    Returns:
        int: The mapped integer value. Returns 1 for positive answers (e.g., 'yes', 'true'), 
                0 for negative answers (e.g., 'no', 'false'), and -1 for other cases.
    """
    model_answer = model_response.lower().strip("'").strip('"')
    if " yes," in model_answer or " yes " in model_answer or model_answer.startswith("yes") or " yes." in model_answer or " knows " in model_answer or model_answer.lower().startswith("true"):
        return 1
    elif " no," in model_answer or " no " in model_answer or model_answer.startswith("no") or " no." in model_answer or " does not know " in model_answer or " doesn't know " in model_answer or model_answer.lower().startswith("false"):
        return 0
    else:
        return -1

def evaluate_binary_q_with_f1(qa, model_response):
    """
    Evaluates a binary question with F1 score.

    Args:
        qa (dict): A dictionary containing the question and correct answer.
        model_response (str): The response generated by the model.

    Returns:
        bool: True if the model's response contains the correct answer, False otherwise.
    """
    tom_answer = qa['correct_answer'].split(":")[0] # for no:long
    model_answer = model_response.split()[0].lower().strip(",")
    if tom_answer in model_answer:
        return True
    else:
        return False

def evaluate_fact_q(qa, model_response):
    result = compute_f1(qa['correct_answer'].lower(), model_response.lower())
    return result

def yesno_to_int(yesno_str):
    mapping = {'yes': 1, 'no': 0, 'no:long': 0, 'error': -1}
    return mapping[yesno_str]


def evaluate_fantom(qas, predictions):
    """
    Evaluates the model's response for a list of questions and predictions.

    Args:
        qas (list): List of question-answer pairs.
        predictions (list): List of model predictions.

    Returns:
        list: Updated list of question-answer pairs with evaluation results and predictions.
    """
    print("Running evaluation...")
    assert len(qas) == len(predictions), "Number of questions and model predictions should be the same."

    for qa, pred in tqdm(zip(qas, predictions), total=len(qas)):
        if qa['question_type'].startswith("tom:belief:"):
            if qa['question_type'].endswith(":multiple-choice"):
                result = evaluate_mc_belief_q(qa, pred)
            else:
                result, word_overlap = evaluate_belief_q(qa, pred)
                qa['word_overlap'] = word_overlap
        elif qa['question_type'].endswith(":list"):
            result, excluded_aware_character, included_unaware_character = evaluate_list_q(qa, pred)
            qa['excluded_aware_character'] = excluded_aware_character
            qa['included_unaware_character'] = included_unaware_character
        elif qa['question_type'].endswith(":binary"):
            _binary_answer = map_binary_answer_to_int(pred)
            if yesno_to_int(qa['correct_answer']) == _binary_answer:
                result = True
            else:
                result = False
            qa['binarized_model_answer'] = _binary_answer
        elif qa['question_type'].startswith("fact"):
            result = evaluate_fact_q(qa, pred)
        else:
            raise NotImplementedError

        qa['result'] = result
        qa['prediction'] = pred

    return qas


def score_and_analyze(df, model_name, conversation_input_type, aggregation_target, target_scenario='inaccessible'):
    """
    Aggregates scores and performs analysis on the model responses and evaluation results.

    Args:
        df (pandas.DataFrame): The dataframe containing the FANToM QA pairs, model responses, and evaluation results.
        target_scenario (str, optional): The target scenario for analysis. Defaults to 'inaccessible'.

    Returns:
        dict: A dictionary containing the calculated scores and analysis results.
    """
    report = {'model': model_name, 'conversation_input_type': conversation_input_type}
    f1_metric = evaluate.load("f1")
    aggregation_target = aggregation_target + "_id"
    tom_df = df[df['question_type'].str.startswith("tom")].copy()
    target_df = tom_df[tom_df['missed_info_accessibility'] == target_scenario].copy()

    if target_scenario == 'accessible':
        # Filter out the set_ids that have all the questions that are labeled as accessible for the ALL* and ALL scores
        # This is because in sets where there are belief questions labeled as 'inaccessible' (i.e., there is an unaware character), all the other question types are also treated as 'inaccessible'.
        # As a result, in the accessible scenario, there are many sets that are only left with a few belief questions. This leads to exaggerated ALl* and ALL scores.
        # As a quick & dirty solution, we will focus only on the sets where all the questions are labeled as accessible when measuring the the ALL* and ALL scores.
        _target_df = tom_df[tom_df['missed_info_accessibility'] == target_scenario].copy()
        set_ids = _target_df['set_id'].unique()
        target_sets = []
        for set_id in set_ids:
            if tom_df[tom_df['set_id'] == set_id]['missed_info_accessibility'].eq(target_scenario).all():
                target_sets.append(set_id)
    else:
        target_sets = target_df['set_id'].unique()


    ############# Scores #############
    # ALL* score
    report[target_scenario+':set:ALL*'] = target_df[target_df['set_id'].isin(target_sets)].groupby(aggregation_target)['result'].all().mean()

    # ALL score
    target_question_for_all = ["tom:belief:"+target_scenario+":multiple-choice", "tom:answerability:list", "tom:answerability:binary", "tom:info_accessibility:list", "tom:info_accessibility:binary"]
    report[target_scenario+':set:ALL'] = target_df[target_df['question_type'].isin(target_question_for_all) & target_df['set_id'].isin(target_sets)].groupby(aggregation_target)['result'].all().mean()

    # Belief Questions: multiple-choice, dist., f1
    report[target_scenario+':belief:multiple-choice'] = target_df[target_df['question_type'].str.endswith(":multiple-choice")]['result'].mean()
    report[target_scenario+':belief:distance'] = target_df[target_df['question_type'] == "tom:belief:"+target_scenario]['result'].mean()
    report[target_scenario+':belief_true_word-f1'] = target_df[(target_df['question_type'] == "tom:belief:"+target_scenario) & (target_df['result'] == True)]['word_overlap'].mean()

    # Answerability Questions: ALL, list, binary
    report[target_scenario+':answerability:set:ALL'] = target_df[target_df['question_type'].str.startswith("tom:answerability")].groupby(aggregation_target)['result'].all().mean()
    report[target_scenario+':answerability:list'] = target_df[target_df['question_type'] == "tom:answerability:list"]['result'].mean()
    answerability_model_responses = target_df[target_df['question_type'] == 'tom:answerability:binary']['binarized_model_answer'].to_list()
    answerability_references = target_df[target_df['question_type'] == 'tom:answerability:binary']['correct_answer'].map(yesno_to_int).to_list()
    report[target_scenario+':answerability:binary-f1'] = f1_metric.compute(predictions=answerability_model_responses, references=answerability_references, pos_label=0, average="weighted")['f1']

    # Info Accessibility Questions: All, list, binary
    report[target_scenario+':info_accessibility:set:ALL'] = target_df[target_df['question_type'].str.startswith("tom:info_accessibility")].groupby(aggregation_target)['result'].all().mean()
    report[target_scenario+':info_accessibility:list'] = target_df[target_df['question_type']=="tom:info_accessibility:list"]['result'].mean()
    accessibility_model_responses = target_df[target_df['question_type'] == 'tom:info_accessibility:binary']['binarized_model_answer'].to_list()
    accessibility_references = target_df[target_df['question_type'] == 'tom:info_accessibility:binary']['correct_answer'].map(yesno_to_int).to_list()
    report[target_scenario+':info_accessibility:binary-f1'] = f1_metric.compute(predictions=accessibility_model_responses, references=accessibility_references, pos_label=0, average="weighted")['f1']

    # Fact Questions
    report['fact_word-f1'] = df[df['question_type'].str.startswith("fact")]['result'].mean()


    ############# Error Analysis #############
    # why the model got the list questions wrong
    list_wrong = target_df[(target_df['question_type']=="tom:answerability:list") & (target_df['result'] == False)][['excluded_aware_character', 'included_unaware_character']].copy()
    list_wrong['both'] = list_wrong['excluded_aware_character'] & list_wrong['included_unaware_character']
    list_wrong['reason'] = list_wrong.apply(lambda x: 'did_both' if x['both'] else 'excluded_aware_character' if x['excluded_aware_character'] else 'included_unaware_character', axis=1)
    report[target_scenario+':tom:lists:wrong_reasons:freq'] = list_wrong['reason'].value_counts(normalize=False).to_dict()

    # why the model got the binary questions wrong
    binary_wrong_reasons = target_df[(target_df['question_type'].str.endswith(":binary")) & (target_df['result'] == False)]['binarized_model_answer'].value_counts(normalize=False).to_dict()
    if 0 in binary_wrong_reasons.keys():
        binary_wrong_reasons['false_negative'] = binary_wrong_reasons.pop(0)
    if 1 in binary_wrong_reasons.keys():
        binary_wrong_reasons['false_positive'] = binary_wrong_reasons.pop(1)
    if -1 in binary_wrong_reasons.keys():
        binary_wrong_reasons['irrelevant_response'] = binary_wrong_reasons.pop(-1)
    report[target_scenario+':tom:binary:wrong_reasons:freq'] = binary_wrong_reasons


    ############# More Analysis #############
    # 1. Results for each tom_order type in Belief questions: first order and second order
    belief_df = tom_df[tom_df['question_type'] == ('tom:belief:' + target_scenario)].copy() # XXX: only consider the BeliefQ[dist.] questions
    belief_df['tom_order'] = belief_df['tom_type'].map(lambda x: x.split(":")[0])
    tom_order_results = belief_df.groupby('tom_order')['result'].value_counts(normalize=True)
    for idx in tom_order_results.index:
        if idx[1] == True:
            report[target_scenario + ":" + idx[0]] = tom_order_results[idx]

    # 2. Cyclic vs Acyclic second order belief questions
    belief_results = belief_df.groupby('tom_type')['result'].value_counts(normalize=True)
    for idx in belief_results.index:
        if idx[1] == True:
            report[target_scenario + ":" + idx[0]] = belief_results[idx]

    # 3. Character tracking analysis 
    binary_qas = target_df[(target_df['question_type'].str.endswith(":binary"))].copy()
    binary_qas['target_character'] = binary_qas['question'].map(lambda x: x.removeprefix("Does ").split(" know")[0].lower())
    belief_qas = target_df[(target_df['question_type'].str.startswith("tom:belief"))].copy()
    belief_qas['target_character'] = belief_qas['question'].map(lambda x: x.lower().split("does ")[1].split()[0].lower())
    answerability_list_qas = target_df[target_df['question_type'].str.endswith("answerability:list")].set_index(aggregation_target, drop=False)
    accessibility_list_qas = target_df[target_df['question_type'].str.endswith("info_accessibility:list")].set_index(aggregation_target, drop=False)

    # Tile the list question responses to the binary question level for each character
    binary_answerability_qas = binary_qas[binary_qas['question_type'].str.startswith('tom:answerability:')]
    tiled_answerability_list_qas = binary_answerability_qas[[aggregation_target, 'target_character', 'correct_answer']].join(answerability_list_qas[['prediction', aggregation_target]], on=aggregation_target, how='outer', lsuffix='-binary')
    tiled_answerability_list_qas['binarized_model_answer'] = tiled_answerability_list_qas.apply(lambda x: x['target_character'].lower() in x['prediction'].lower() if pd.notnull(x['prediction']) and pd.notnull(x['target_character']) else False, axis=1)
    tiled_answerability_list_qas['binarized_correct_answer'] = tiled_answerability_list_qas['correct_answer'].map(lambda x: True if x =='yes' else False)
    tiled_answerability_list_qas['result'] = tiled_answerability_list_qas.apply(lambda x: x['binarized_model_answer'] == x['binarized_correct_answer'], axis=1)

    binary_accessibility_qas = binary_qas[binary_qas['question_type'].str.startswith('tom:info_accessibility:')]
    tiled_accessibility_list_qas = binary_accessibility_qas[[aggregation_target, 'target_character', 'correct_answer']].join(accessibility_list_qas[['prediction', aggregation_target]], on=aggregation_target, how='outer', lsuffix='-binary')
    tiled_accessibility_list_qas['binarized_model_answer'] = tiled_accessibility_list_qas.apply(lambda x: x['target_character'].lower() in x['prediction'].lower() if pd.notnull(x['prediction']) and pd.notnull(x['target_character']) else False, axis=1)
    tiled_accessibility_list_qas['binarized_correct_answer'] = tiled_accessibility_list_qas['correct_answer'].map(lambda x: True if x =='yes' else False)
    tiled_accessibility_list_qas['result'] = tiled_accessibility_list_qas.apply(lambda x: x['binarized_model_answer'] == x['binarized_correct_answer'], axis=1)

    df_for_all_character_metric = pd.concat([binary_qas[['target_character', aggregation_target, 'result']], belief_qas[['target_character', aggregation_target, 'result']], tiled_answerability_list_qas[['target_character', aggregation_target, 'result']], tiled_accessibility_list_qas[['target_character', aggregation_target, 'result']]])
    report[target_scenario+':set:ALL_character'] = df_for_all_character_metric.groupby([aggregation_target, 'target_character'])['result'].all().mean()
    df_for_character_consistency = pd.concat([binary_qas[['target_character', aggregation_target, 'binarized_model_answer']], tiled_answerability_list_qas[['target_character', aggregation_target, 'binarized_model_answer']], tiled_accessibility_list_qas[['target_character', aggregation_target, 'binarized_model_answer']]])
    report[target_scenario+':set:character_answer_consistency'] = df_for_character_consistency.groupby([aggregation_target, 'target_character'])['binarized_model_answer'].nunique().eq(1).mean() # how often the model gives the "same answer" for the binary and list questions for the same character

    for k, v in report.items():
        if isinstance(v, float):
            report[k] = round(v, 3) * 100

    return report


def run_reports(qa_results, aggregation_target, conversation_input_type, model_name):
    """
    Create report after scoring and analyzing the results

    Input:
    - qa_results: a list of qa results

    Output:
    - report: a dictionary of scores and analysis

    Note:
    We can further increase the difficulty of the task by changing the aggregation target from 'set_id' to 'part_id' or 'conversation_id'.
    A conversation part refers to the brief section of the conversation that is the relevant part to the question.
    Each conversation part comprises multiple sets of questions, and every conversation consists of multiple conversation parts.
    For instance, if you designate 'part_id' as the aggregation target, the ALL scores will be aggregated for each individual part of the conversation.
    This adjustment will result in the ALL score being aggregated across multiple sets.

    Currently, the default conversation-input-type is 'short' and the ALL scores are aggregated for each set of questions (i.e., aggregation-target to 'set'), which will be the easiest setup for the models.
    The most difficult setup will be to give the full conversation input to the model (i.e., conversation-input-type to 'full') and aggregate the ALL scores for each conversation (i.e., aggregation-target to 'conversation_id')
    """
    df = pd.DataFrame(qa_results)

    # Drop binary questions with no:long answer when input type is short
    if conversation_input_type == "short":
        df.drop(df[(df['question_type'].str.endswith(":binary")) & (df['correct_answer'] == 'no:long')].index, inplace=True)

    df['conversation_id'] = df['set_id'].map(lambda x: x.split("-")[0])
    df['part_id'] = df['set_id'].map(lambda x: "-".join(x.split("-")[:2]))

# score_and_analyze(df, model_name, conversation_input_type, aggregation_target, target_scenario='inaccessible'):
    report = score_and_analyze(df, model_name, conversation_input_type, aggregation_target, target_scenario='inaccessible')
    control_question_report = score_and_analyze(df, model_name, conversation_input_type, aggregation_target, target_scenario='accessible')
    reports = {'fantom': report, 'control_task': control_question_report}

    print("\n[[ FANToM input type: {} ]]".format(conversation_input_type))
    print("[[ Model: {} ]]\n".format(model_name))
    for k, v in reports['fantom'].items():
        print(k, ":", v, file=sys.stderr)
        print()

    return reports




### Evaluation utils for Bigtom


def evaluate_bigtom(inputs, model_responses):

    stats = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {'valid': 0 ,'correct': 0, 'total': 0})))
    total_correct = 0
    total_count = 0
    total_valid_responses = 0

    for idx, item in enumerate(inputs):
        meta = item.get('meta_data', {})
        cond = meta.get('condition')
        var = meta.get('variable')
        ib = meta.get('init_belief')

        answer_key = item['answer'].split(')')[0]
        if 'a' in answer_key:
            true_answer_key = '(a)'
        elif 'b' in answer_key:
            true_answer_key = '(b)'
        else:
            continue  # skip bad format


        answer_option_a = item["options"][0].split(')')[-1]
        answer_option_b = item["options"][1].split(')')[-1]
        true_answer_sentence = item['answer'].split(')')[-1]


        model_response = model_responses[idx].lower().strip()

        stats[cond][var][ib]['total'] += 1
        correct = true_answer_key in model_response.lower()
        correct_sentence = true_answer_sentence.lower() in model_response.lower()
        if correct or correct_sentence:
        # if correct:
            stats[cond][var][ib]['correct'] += 1

        total_count += 1
        if correct:
            total_correct += 1

        # check for valid format
        answer_options = ['(a)', '(b)', answer_option_a.lower(), answer_option_b.lower()]
        # answer_options = ['(a)', '(b)']
        if any(a_opt in model_response.lower() for a_opt in answer_options):
            total_valid_responses += 1
            stats[cond][var][ib]['valid'] += 1


       

    # Print summary
    print("Condition | Variable | InitBelief | Valid | Correct | Total | Accuracy")
    print("-" * 65)
    for cond in stats:
        for var in stats[cond]:
            for ib in stats[cond][var]:
                s = stats[cond][var][ib]
                acc = (s['correct'] / s['total']) if s['total'] else 0
                print(f"{cond:10} | {var:15} | {ib:10} | {s['valid']:7} | {s['correct']:7} | {s['total']:5} | {acc:7.1%}", file=sys.stderr)
    
    overall_acc = total_correct / total_count if total_count > 0 else 0
    valid_response_acc = total_valid_responses / total_count if total_count > 0 else 0
    print(f"Overall accuracy: {overall_acc:.2%}", file=sys.stderr)
    print(f"Valid response accuracy: {valid_response_acc:.2%}", file=sys.stderr)
    return stats




### Evaluation utils for Hitom

def evaluate_hitom(inputs, model_responses):
    print('ff')