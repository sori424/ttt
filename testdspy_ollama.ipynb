{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:08:42.490584Z",
     "start_time": "2025-08-28T11:08:37.420977Z"
    }
   },
   "source": [
    "import dspy, os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"  # DSPy expects a key; vLLM ignores it\n",
    "lm = dspy.LM('ollama_chat/phi4-mini', api_base='http://localhost:11434', api_key='', temperature=0.7)\n",
    "dspy.configure(lm=lm)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:08:50.076610Z",
     "start_time": "2025-08-28T11:08:42.498101Z"
    }
   },
   "source": [
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Disable adapters so JSON mode isn't triggered\n",
    "generate_answer = dspy.Predict(BasicQA, enforce_schema=False)\n",
    "\n",
    "result = generate_answer(question=\"What is a fungus?\")\n",
    "print(result.answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fungus\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:08:50.145449Z",
     "start_time": "2025-08-28T11:08:50.139267Z"
    }
   },
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts and psychological insights\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "\n",
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Generate a logical rule query based on the context to answer the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts and psychological insights\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()\n",
    "\n",
    "\n",
    "def deduplicate(seq: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "        From Raymond Hettinger\n",
    "        https://twitter.com/raymondh/status/944125570534621185\n",
    "        Since Python 3.6 Dict are ordered\n",
    "        Benchmark: https://gist.github.com/peterbe/67b9e40af60a1d5bcb1cfb4b2937b088\n",
    "    \"\"\"\n",
    "    return list(dict.fromkeys(seq))\n",
    "\n",
    "class MultiHopSearchWithPoT(dspy.Module):\n",
    "    def __init__(self, num_hops):\n",
    "        self.num_hops = num_hops\n",
    "        self.generate_query = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        for _ in range(self.num_hops):\n",
    "            query = self.generate_query(context=context, question=question).query\n",
    "            context = deduplicate(context + [query])\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:08:53.957676Z",
     "start_time": "2025-08-28T11:08:50.150965Z"
    }
   },
   "source": [
    "multi_hop_pot = MultiHopSearchWithPoT(num_hops=2)\n",
    "question = (\n",
    "    \"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. Noor believes that the milk pitcher contains oatmilk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. What will Noor do?\",\n",
    ")\n",
    "multi_hop_pot(question=question).answer"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not know'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:08:53.975973Z",
     "start_time": "2025-08-28T11:08:53.972709Z"
    }
   },
   "source": "dspy.inspect_history(n=1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:08:53.951970]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `context` (str): may contain relevant facts and psychological insights\n",
      "2. `question` (str):\n",
      "Your output fields are:\n",
      "1. `answer` (str): often between 1 and 5 words\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "Inputs will have the following structure:\n",
      "\n",
      "[[ ## context ## ]]\n",
      "{context}\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "Outputs will be a JSON object with the following fields.\n",
      "\n",
      "{\n",
      "  \"answer\": \"{answer}\"\n",
      "}\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer questions with short factoid answers.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## context ## ]]\n",
      "[1] «\"Will Noor know whether or not it's actually almond milk in the cup?\"»\n",
      "[2] «««\n",
      "    \"Will Noor know whether the cup contains actual almond milk?\"\n",
      "    \n",
      "    ([[## completed##\n",
      "    ]]\n",
      "»»»\n",
      "\n",
      "[[ ## question ## ]]\n",
      "[\"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. Noor believes that the milk pitcher contains oatmilk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. What will Noor do?\"]\n",
      "\n",
      "Respond with a JSON object in the following order of fields: `answer`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m{\n",
      "  \"answer\": \"Not know\"\n",
      "}\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:09:45.762959Z",
     "start_time": "2025-08-28T11:08:53.989119Z"
    }
   },
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Event(BaseModel):\n",
    "    order: int\n",
    "    actor: str\n",
    "    action: str\n",
    "    belief: str\n",
    "    location: str | None = None\n",
    "\n",
    "class ToMSignature(dspy.Signature):\n",
    "    \"\"\"Extract implicit reasoning needed for a ToM question, then answer.\"\"\"\n",
    "    story: str = dspy.InputField(desc=\"Short story/problem statement\")\n",
    "\n",
    "    world_facts: List[str] = dspy.OutputField()\n",
    "    timeline: List[Event] = dspy.OutputField()\n",
    "    bridging_rules: List[str] = dspy.OutputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "\n",
    "class ToMExtractor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = dspy.Predict(ToMSignature)\n",
    "\n",
    "        def reward_fn(args, pred) -> float:\n",
    "            score = 0.0\n",
    "            # 1) final answer present\n",
    "            if getattr(pred, \"answer\", \"\").strip():\n",
    "                score += 1\n",
    "    \n",
    "            return score\n",
    "\n",
    "        # Try up to N times; stop when score >= threshold\n",
    "        self.extract = dspy.Refine(\n",
    "            module=base,\n",
    "            N=4,\n",
    "            reward_fn=reward_fn,\n",
    "            threshold=2.5,   # require any 2–3 of the checks to pass\n",
    "        )\n",
    "\n",
    "    def forward(self, story: str):\n",
    "        return self.extract(story=story)\n",
    "\n",
    "\n",
    "\n",
    "# 4) Run it on your ToM example\n",
    "story = (\n",
    "  \"Noor is working as a barista at a busy coffee shop. Noor wants to make a \"\n",
    "  \"delicious cappuccino for a customer who asked for oat milk. Noor grabs a \"\n",
    "  \"milk pitcher and fills it with oat milk. \"\n",
    "  \"A coworker, who didn't hear the customer's request, \"\n",
    "  \"swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. \"\n",
    "  \"What will Noor do?\"\n",
    ")\n",
    "\n",
    "extractor = ToMExtractor()\n",
    "result = extractor(story=story)\n",
    "\n",
    "# 5) Inspect the structured “reasoning steps”\n",
    "print(\"World facts:\", *result.world_facts, sep=\"\\n- \")\n",
    "print(\"\\nTimeline:\", *[e.model_dump() for e in result.timeline], sep=\"\\n- \")\n",
    "print(\"\\nBridging rules:\", *result.bridging_rules, sep=\"\\n- \")\n",
    "print(\"\\nFinal answer:\", result.answer)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refine: Attempt failed with temperature 0.5: 1 validation error for dict[str,str]\n",
      "self\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "Refine: Attempt failed with temperature 0.625: 1 validation error for dict[str,str]\n",
      "self\n",
      "  Input should be a valid string [type=string_type, input_value=[\"In scenarios where cust...per customer's order?'\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "World facts:\n",
      "- Noor is working at a coffee shop.\n",
      "- A cappuccino customer ordered oat milk.\n",
      "- Coworkers are present in the coffee shop.\n",
      "- The coworker swapped almond for oat milk without Noor's knowledge.\n",
      "\n",
      "Timeline:\n",
      "- {'order': 1, 'actor': 'Noor', 'action': 'grabs pitcher of milk and fills it with oat milk.', 'belief': 'The customer asked for an oat cappuccino.', 'location': None}\n",
      "- {'order': 2, 'actor': 'coworker', 'action': 'swaps almond milk in the pitcher with oat milk.', 'belief': \"Noor didn't hear about customer's request and swapped what was there (almond) instead of replacing it correctly based on Noor's preparation action.\", 'location': None}\n",
      "\n",
      "Bridging rules:\n",
      "- If an event occurs after another, then a chain reaction can happen\n",
      "- If the belief is incorrect but not known by both parties involved in communication or actions\n",
      "- An unintentional swap may occur if coworker was unaware of customer's request.\n",
      "\n",
      "Final answer: Noor will likely make and serve almond milk cappuccino to the customer instead of oat milk cappuccino as intended, since she did not realize her pitcher had been swapped with almond milk by a coworker.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:10:44.357716Z",
     "start_time": "2025-08-28T11:09:45.785412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Event(BaseModel):\n",
    "    order: int\n",
    "    actor: str\n",
    "    action: str\n",
    "    belief: str\n",
    "    location: str | None = None\n",
    "\n",
    "class ToMSignature(dspy.Signature):\n",
    "    \"\"\"Extract implicit reasoning needed for a ToM question, then answer.\"\"\"\n",
    "    story: str = dspy.InputField(desc=\"Short story/problem statement\")\n",
    "    choices: list[str] = dspy.InputField()\n",
    "\n",
    "    world_facts: List[str] = dspy.OutputField()\n",
    "    timeline: List[Event] = dspy.OutputField()\n",
    "    bridging_rules: List[str] = dspy.OutputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "\n",
    "class ToMExtractor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = dspy.Predict(ToMSignature)\n",
    "\n",
    "        def reward_fn(args, pred) -> float:\n",
    "            score = 0.0\n",
    "            # 1) final answer present\n",
    "            if getattr(pred, \"answer\", \"\").strip():\n",
    "                score += 1\n",
    "\n",
    "            return score\n",
    "\n",
    "        # Try up to N times; stop when score >= threshold\n",
    "        self.extract = dspy.Refine(\n",
    "            module=base,\n",
    "            N=4,\n",
    "            reward_fn=reward_fn,\n",
    "            threshold=2.5,   # require any 2–3 of the checks to pass\n",
    "        )\n",
    "\n",
    "    def forward(self, story: str, choices: List[str]):\n",
    "        return self.extract(story=story, choices=choices)\n",
    "\n",
    "\n",
    "\n",
    "# 4) Run it on your ToM example\n",
    "story = (\n",
    "  \"Noor is working as a barista at a busy coffee shop. Noor wants to make a \"\n",
    "  \"delicious cappuccino for a customer who asked for oat milk. Noor grabs a \"\n",
    "  \"milk pitcher and fills it with oat milk. \"\n",
    "  \"A coworker, who didn't hear the customer's request, \"\n",
    "  \"swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. \"\n",
    "  \"Noor sees her coworker swapping the milk.\"\n",
    "  \"What will Noor do?\"\n",
    ")\n",
    "choices = [\"(a) Noor believes the milk pitcher contains almond milk.\",\n",
    "           \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
    "]\n",
    "\n",
    "extractor = ToMExtractor()\n",
    "result = extractor(story=story, choices=choices)\n",
    "\n",
    "# 5) Inspect the structured “reasoning steps”\n",
    "print(\"World facts:\", *result.world_facts, sep=\"\\n- \")\n",
    "print(\"\\nTimeline:\", *[e.model_dump() for e in result.timeline], sep=\"\\n- \")\n",
    "print(\"\\nBridging rules:\", *result.bridging_rules, sep=\"\\n- \")\n",
    "print(\"\\nFinal answer:\", result.answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World facts:\n",
      "- Noor is working at a coffee shop.\n",
      "- A customer requested oat milk for their cappuccino.\n",
      "- Noor grabbed an empty pitcher and filled it with almond milk instead of the customer's request.\n",
      "\n",
      "Timeline:\n",
      "- {'order': 1, 'actor': 'Customer', 'action': 'Requested oat milk', 'belief': 'The customer wants a cappuccino made from their requested ingredients, which include oat milk.', 'location': None}\n",
      "- {'order': 2, 'actor': 'Noor', 'action': 'Filled pitcher with almond milk', 'belief': \"Assumed that the customer's request for oat milk was fulfilled without verifying the type of milk used in preparation.\", 'location': 'coffee shop'}\n",
      "\n",
      "Bridging rules:\n",
      "- If a customer requests specific ingredients and an actor prepares food based on this input, then it is likely (but not guaranteed) that those requested components are included unless there’s evidence to the contrary.\n",
      "- When observing someone else preparing something without knowing their intentions or knowledge about what they have done wrong.\n",
      "\n",
      "Final answer: (a) Noor believes the milk pitcher contains almond milk.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:11:38.648155Z",
     "start_time": "2025-08-28T11:10:44.394126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Run it on your ToM example\n",
    "story = (\n",
    "\"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\n\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?\"\n",
    "  \"What will Noor do?\"\n",
    ")\n",
    "choices = [\"(a) Noor believes the milk pitcher contains almond milk.\",\n",
    "           \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
    "]\n",
    "\n",
    "extractor = ToMExtractor()\n",
    "result = extractor(story=story, choices=choices)\n",
    "\n",
    "# 5) Inspect the structured “reasoning steps”\n",
    "print(\"World facts:\", *result.world_facts, sep=\"\\n- \")\n",
    "print(\"\\nTimeline:\", *[e.model_dump() for e in result.timeline], sep=\"\\n- \")\n",
    "print(\"\\nBridging rules:\", *result.bridging_rules, sep=\"\\n- \")\n",
    "print(\"\\nFinal answer:\", result.answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refine: Attempt failed with temperature 0.7: 2 validation errors for dict[str,str]\n",
      "concrete_scenarios_and_mistakes\n",
      "  Input should be a valid string [type=string_type, input_value=['[...] If Noor grabbed a...stance is in it. [...]'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "future_different_actions_advice\n",
      "  Input should be a valid string [type=string_type, input_value={'(a) Noor believes the m...hanges made by others.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "World facts:\n",
      "- Noor is working at a coffee shop.\n",
      "- A customer asked for oat milk in their cappuccino order.\n",
      "- Noor grabbed the pitcher and filled it with oat milk.\n",
      "\n",
      "Timeline:\n",
      "- {'order': 1, 'actor': 'Noor', 'action': 'grabs pitcher, fills with oat milk', 'belief': 'the customer asked for oat milk in their cappuccino order.', 'location': None}\n",
      "- {'order': 2, 'actor': 'coworker', 'action': 'swaps the contents of the pitcher from oat to almond milk', 'belief': '', 'location': None}\n",
      "\n",
      "Bridging rules:\n",
      "- If Noor grabbed a pitcher and filled it with what she believed was required for an order, then she will believe that the pitcher contains whatever substance is in it.\n",
      "- Noor saw her coworker swap almond milk into the pitcher's contents.\n",
      "\n",
      "Final answer: (a) Noor believes the milk pitcher contains almond milk.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:11:38.673330Z",
     "start_time": "2025-08-28T11:11:38.669656Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    world_facts=['Noor is working at a coffee shop.', 'A customer asked for oat milk in their cappuccino order.', 'Noor grabbed the pitcher and filled it with oat milk.'],\n",
       "    timeline=[Event(order=1, actor='Noor', action='grabs pitcher, fills with oat milk', belief='the customer asked for oat milk in their cappuccino order.', location=None), Event(order=2, actor='coworker', action='swaps the contents of the pitcher from oat to almond milk', belief='', location=None)],\n",
       "    bridging_rules=['If Noor grabbed a pitcher and filled it with what she believed was required for an order, then she will believe that the pitcher contains whatever substance is in it.', \"Noor saw her coworker swap almond milk into the pitcher's contents.\"],\n",
       "    answer='(a) Noor believes the milk pitcher contains almond milk.'\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:11:38.695587Z",
     "start_time": "2025-08-28T11:11:38.691511Z"
    }
   },
   "cell_type": "code",
   "source": "dspy.inspect_history(n=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:11:21.832314]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `story` (str): Short story/problem statement\n",
      "2. `choices` (list[str]): \n",
      "3. `hint_` (str): A hint to the module from an earlier run\n",
      "Your output fields are:\n",
      "1. `world_facts` (list[str]): \n",
      "2. `timeline` (list[Event]): \n",
      "3. `bridging_rules` (list[str]): \n",
      "4. `answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "Inputs will have the following structure:\n",
      "\n",
      "[[ ## story ## ]]\n",
      "{story}\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "{choices}\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "{hint_}\n",
      "\n",
      "Outputs will be a JSON object with the following fields.\n",
      "\n",
      "{\n",
      "  \"world_facts\": \"{world_facts}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"timeline\": \"{timeline}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"$defs\\\": {\\\"Event\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"action\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Action\\\"}, \\\"actor\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Actor\\\"}, \\\"belief\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Belief\\\"}, \\\"location\\\": {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}], \\\"default\\\": null, \\\"title\\\": \\\"Location\\\"}, \\\"order\\\": {\\\"type\\\": \\\"integer\\\", \\\"title\\\": \\\"Order\\\"}}, \\\"required\\\": [\\\"order\\\", \\\"actor\\\", \\\"action\\\", \\\"belief\\\"], \\\"title\\\": \\\"Event\\\"}}, \\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Event\\\"}}\",\n",
      "  \"bridging_rules\": \"{bridging_rules}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"answer\": \"{answer}\"\n",
      "}\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract implicit reasoning needed for a ToM question, then answer.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## story ## ]]\n",
      "Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "[\"(a) Noor believes the milk pitcher contains almond milk.\", \"(b) Noor believes the milk pitcher contains oat milk.\"]\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "(a) Noor should trust what she sees in front of her, which would lead to believing that the pitcher contains almond milk. To avoid confusion like this happening again, Noor could consider implementing double-checking mechanisms for crucial tasks such as fulfilling customer orders.\n",
      "\n",
      "Respond with a JSON object in the following order of fields: `world_facts` (must be formatted as a valid Python list[str]), then `timeline` (must be formatted as a valid Python list[Event]), then `bridging_rules` (must be formatted as a valid Python list[str]), then `answer`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m{\n",
      "\"world_facts\": [\n",
      "\"Noor is working at a busy coffee shop.\",\n",
      "\"Noor wants to make an oat milk cappuccino for the customer.\"\n",
      "],\n",
      "\"timeline\": [\n",
      "{ \"order\": 1, \"actor\": \"Noor\", \"action\": \"grabs pitcher and fills it with almond milk.\", \"belief\": \"The pitcher contains almond milk.\" , \"location\": null },\n",
      "{ \"order\": 2, \"actor\": \"Coworker\", \"action\": \"swaps oat for almond in the pitcher while Noor is attending to another task. \", \"belief\": \"Swapping of oak and almond has been done.\", \"location\": null }\n",
      "],\n",
      "\"bridging_rules\": [\n",
      "\"If a coworker's action contradicts what was seen, then the belief should align with what's currently observed.\",\n",
      "\"A belief that aligns with current observation trumps previous knowledge.\"\n",
      "], \"answer\": \"(a) Noor believes the milk pitcher contains almond milk.\"}\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:11:25.684055]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `program_code` (str): The code of the program that we are analyzing\n",
      "2. `modules_defn` (str): The definition of each module in the program, including its I/O\n",
      "3. `program_inputs` (str): The inputs to the program that we are analyzing\n",
      "4. `program_trajectory` (str): The trajectory of the program's execution, showing each module's I/O\n",
      "5. `program_outputs` (str): The outputs of the program that we are analyzing\n",
      "6. `reward_code` (str): The code of the reward function that we are analyzing\n",
      "7. `target_threshold` (float): The target threshold for the reward function\n",
      "8. `reward_value` (float): The reward value assigned to the program's outputs\n",
      "9. `module_names` (list[str]): The names of the modules in the program, for which we seek advice\n",
      "Your output fields are:\n",
      "1. `discussion` (str): Discussing blame of where each module went wrong, if it did\n",
      "2. `advice` (dict[str, str]): For each module, describe very concretely, in this order: the specific scenarios in which it has made mistakes in the past and what each mistake was, followed by what it should do differently in that kind ofscenario in the future. If the module is not to blame, write N/A.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## program_code ## ]]\n",
      "{program_code}\n",
      "\n",
      "[[ ## modules_defn ## ]]\n",
      "{modules_defn}\n",
      "\n",
      "[[ ## program_inputs ## ]]\n",
      "{program_inputs}\n",
      "\n",
      "[[ ## program_trajectory ## ]]\n",
      "{program_trajectory}\n",
      "\n",
      "[[ ## program_outputs ## ]]\n",
      "{program_outputs}\n",
      "\n",
      "[[ ## reward_code ## ]]\n",
      "{reward_code}\n",
      "\n",
      "[[ ## target_threshold ## ]]\n",
      "{target_threshold}\n",
      "\n",
      "[[ ## reward_value ## ]]\n",
      "{reward_value}\n",
      "\n",
      "[[ ## module_names ## ]]\n",
      "{module_names}\n",
      "\n",
      "[[ ## discussion ## ]]\n",
      "{discussion}\n",
      "\n",
      "[[ ## advice ## ]]\n",
      "{advice}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        In the discussion, assign blame to each module that contributed to the final reward being below the threshold, if\n",
      "        any. Then, prescribe concrete advice of how the module should act on its future input when we retry the process, if\n",
      "        it were to receive the same or similar inputs. If a module is not to blame, the advice should be N/A.\n",
      "        The module will not see its own history, so it needs to rely on entirely concrete and actionable advice from you\n",
      "        to avoid the same mistake on the same or similar inputs.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## program_code ## ]]\n",
      "class Predict(Module, Parameter):\n",
      "    def __init__(self, signature: str | type[Signature], callbacks: list[BaseCallback] | None = None, **config):\n",
      "        super().__init__(callbacks=callbacks)\n",
      "        self.stage = random.randbytes(8).hex()\n",
      "        self.signature = ensure_signature(signature)\n",
      "        self.config = config\n",
      "        self.reset()\n",
      "\n",
      "    def reset(self):\n",
      "        self.lm = None\n",
      "        self.traces = []\n",
      "        self.train = []\n",
      "        self.demos = []\n",
      "\n",
      "    def dump_state(self):\n",
      "        state_keys = [\"traces\", \"train\"]\n",
      "        state = {k: getattr(self, k) for k in state_keys}\n",
      "\n",
      "        state[\"demos\"] = []\n",
      "        for demo in self.demos:\n",
      "            demo = demo.copy()\n",
      "\n",
      "            for field in demo:\n",
      "                # FIXME: Saving BaseModels as strings in examples doesn't matter because you never re-access as an object\n",
      "                demo[field] = serialize_object(demo[field])\n",
      "\n",
      "            state[\"demos\"].append(demo)\n",
      "\n",
      "        state[\"signature\"] = self.signature.dump_state()\n",
      "        state[\"lm\"] = self.lm.dump_state() if self.lm else None\n",
      "        return state\n",
      "\n",
      "    def load_state(self, state: dict) -> \"Predict\":\n",
      "        \"\"\"Load the saved state of a `Predict` object.\n",
      "\n",
      "        Args:\n",
      "            state: The saved state of a `Predict` object.\n",
      "\n",
      "        Returns:\n",
      "            Self to allow method chaining.\n",
      "        \"\"\"\n",
      "        excluded_keys = [\"signature\", \"extended_signature\", \"lm\"]\n",
      "        for name, value in state.items():\n",
      "            # `excluded_keys` are fields that go through special handling.\n",
      "            if name not in excluded_keys:\n",
      "                setattr(self, name, value)\n",
      "\n",
      "        self.signature = self.signature.load_state(state[\"signature\"])\n",
      "        self.lm = LM(**state[\"lm\"]) if state[\"lm\"] else None\n",
      "\n",
      "        if \"extended_signature\" in state:  # legacy, up to and including 2.5, for CoT.\n",
      "            raise NotImplementedError(\"Loading extended_signature is no longer supported in DSPy 2.6+\")\n",
      "\n",
      "        return self\n",
      "\n",
      "    def _get_positional_args_error_message(self):\n",
      "        input_fields = list(self.signature.input_fields.keys())\n",
      "        return (\n",
      "            \"Positional arguments are not allowed when calling `dspy.Predict`, must use keyword arguments \"\n",
      "            f\"that match your signature input fields: '{', '.join(input_fields)}'. For example: \"\n",
      "            f\"`predict({input_fields[0]}=input_value, ...)`.\"\n",
      "        )\n",
      "\n",
      "    def __call__(self, *args, **kwargs):\n",
      "        if args:\n",
      "            raise ValueError(self._get_positional_args_error_message())\n",
      "\n",
      "        return super().__call__(**kwargs)\n",
      "\n",
      "    async def acall(self, *args, **kwargs):\n",
      "        if args:\n",
      "            raise ValueError(self._get_positional_args_error_message())\n",
      "\n",
      "        return await super().acall(**kwargs)\n",
      "\n",
      "    def _forward_preprocess(self, **kwargs):\n",
      "        # Extract the three privileged keyword arguments.\n",
      "        assert \"new_signature\" not in kwargs, \"new_signature is no longer a valid keyword argument.\"\n",
      "        signature = ensure_signature(kwargs.pop(\"signature\", self.signature))\n",
      "        demos = kwargs.pop(\"demos\", self.demos)\n",
      "        config = dict(**self.config, **kwargs.pop(\"config\", {}))\n",
      "\n",
      "        # Get the right LM to use.\n",
      "        lm = kwargs.pop(\"lm\", self.lm) or settings.lm\n",
      "\n",
      "        if lm is None:\n",
      "            raise ValueError(\n",
      "                \"No LM is loaded. Please configure the LM using `dspy.configure(lm=dspy.LM(...))`. e.g, \"\n",
      "                \"`dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))`\"\n",
      "            )\n",
      "\n",
      "        if isinstance(lm, str):\n",
      "            # Many users mistakenly use `dspy.configure(lm=\"openai/gpt-4o-mini\")` instead of\n",
      "            # `dspy.configure(lm=dspy.LM(\"openai/gpt-4o-mini\"))`, so we are providing a specific error message.\n",
      "            raise ValueError(\n",
      "                f\"LM must be an instance of `dspy.BaseLM`, not a string. Instead of using a string like \"\n",
      "                f\"'dspy.configure(lm=\\\"{lm}\\\")', please configure the LM like 'dspy.configure(lm=dspy.LM(\\\"{lm}\\\"))'\"\n",
      "            )\n",
      "        elif not isinstance(lm, BaseLM):\n",
      "            raise ValueError(f\"LM must be an instance of `dspy.BaseLM`, not {type(lm)}. Received `lm={lm}`.\")\n",
      "\n",
      "        # If temperature is unset or <=0.15, and n > 1, set temperature to 0.7 to keep randomness.\n",
      "        temperature = config.get(\"temperature\") or lm.kwargs.get(\"temperature\")\n",
      "        num_generations = config.get(\"n\") or lm.kwargs.get(\"n\") or lm.kwargs.get(\"num_generations\") or 1\n",
      "\n",
      "        if (temperature is None or temperature <= 0.15) and num_generations > 1:\n",
      "            config[\"temperature\"] = 0.7\n",
      "\n",
      "        if \"prediction\" in kwargs:\n",
      "            if (\n",
      "                isinstance(kwargs[\"prediction\"], dict)\n",
      "                and kwargs[\"prediction\"].get(\"type\") == \"content\"\n",
      "                and \"content\" in kwargs[\"prediction\"]\n",
      "            ):\n",
      "                # If the `prediction` is the standard predicted outputs format\n",
      "                # (https://platform.openai.com/docs/guides/predicted-outputs), we remove it from input kwargs and add it\n",
      "                # to the lm kwargs.\n",
      "                config[\"prediction\"] = kwargs.pop(\"prediction\")\n",
      "\n",
      "        if not all(k in kwargs for k in signature.input_fields):\n",
      "            present = [k for k in signature.input_fields if k in kwargs]\n",
      "            missing = [k for k in signature.input_fields if k not in kwargs]\n",
      "            logger.warning(\n",
      "                \"Not all input fields were provided to module. Present: %s. Missing: %s.\",\n",
      "                present,\n",
      "                missing,\n",
      "            )\n",
      "        return lm, config, signature, demos, kwargs\n",
      "\n",
      "    def _forward_postprocess(self, completions, signature, **kwargs):\n",
      "        pred = Prediction.from_completions(completions, signature=signature)\n",
      "        if kwargs.pop(\"_trace\", True) and settings.trace is not None and settings.max_trace_size > 0:\n",
      "            trace = settings.trace\n",
      "            if len(trace) >= settings.max_trace_size:\n",
      "                trace.pop(0)\n",
      "            trace.append((self, {**kwargs}, pred))\n",
      "        return pred\n",
      "\n",
      "    def _should_stream(self):\n",
      "        stream_listeners = settings.stream_listeners or []\n",
      "        should_stream = settings.send_stream is not None\n",
      "        if should_stream and len(stream_listeners) > 0:\n",
      "            should_stream = any(stream_listener.predict == self for stream_listener in stream_listeners)\n",
      "\n",
      "        return should_stream\n",
      "\n",
      "    def forward(self, **kwargs):\n",
      "        lm, config, signature, demos, kwargs = self._forward_preprocess(**kwargs)\n",
      "\n",
      "        adapter = settings.adapter or ChatAdapter()\n",
      "\n",
      "        if self._should_stream():\n",
      "            with settings.context(caller_predict=self):\n",
      "                completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "        else:\n",
      "            with settings.context(send_stream=None):\n",
      "                completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "\n",
      "        return self._forward_postprocess(completions, signature, **kwargs)\n",
      "\n",
      "    async def aforward(self, **kwargs):\n",
      "        lm, config, signature, demos, kwargs = self._forward_preprocess(**kwargs)\n",
      "\n",
      "        adapter = settings.adapter or ChatAdapter()\n",
      "        if self._should_stream():\n",
      "            with settings.context(caller_predict=self):\n",
      "                completions = await adapter.acall(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "        else:\n",
      "            with settings.context(send_stream=None):\n",
      "                completions = await adapter.acall(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "\n",
      "        return self._forward_postprocess(completions, signature, **kwargs)\n",
      "\n",
      "    def update_config(self, **kwargs):\n",
      "        self.config = {**self.config, **kwargs}\n",
      "\n",
      "    def get_config(self):\n",
      "        return self.config\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}({self.signature})\"\n",
      "\n",
      "\n",
      "[[ ## modules_defn ## ]]\n",
      "--------------------------------------------------------------------------------\n",
      "Module self\n",
      "\tInput Fields:\n",
      "\t\t1. `story` (str): Short story/problem statement\n",
      "\t\t2. `choices` (list[str]):\n",
      "\tOutput Fields:\n",
      "\t\t1. `world_facts` (list[str]): \n",
      "\t\t2. `timeline` (list[Event]): \n",
      "\t\t3. `bridging_rules` (list[str]): \n",
      "\t\t4. `answer` (str):\n",
      "\tOriginal Instructions: \n",
      "\t\tExtract implicit reasoning needed for a ToM question, then answer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[[ ## program_inputs ## ]]\n",
      "{\n",
      "  \"story\": \"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\n\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\",\n",
      "  \"choices\": [\n",
      "    \"(a) Noor believes the milk pitcher contains almond milk.\",\n",
      "    \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "[[ ## program_trajectory ## ]]\n",
      "[\n",
      "  {\n",
      "    \"module_name\": \"self\",\n",
      "    \"inputs\": {\n",
      "      \"story\": \"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\n\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\",\n",
      "      \"choices\": [\n",
      "        \"(a) Noor believes the milk pitcher contains almond milk.\",\n",
      "        \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
      "      ],\n",
      "      \"hint_\": \"(a) Noor should trust what she sees in front of her, which would lead to believing that the pitcher contains almond milk. To avoid confusion like this happening again, Noor could consider implementing double-checking mechanisms for crucial tasks such as fulfilling customer orders.\"\n",
      "    },\n",
      "    \"outputs\": {\n",
      "      \"world_facts\": [\n",
      "        \"Noor is working at a busy coffee shop.\",\n",
      "        \"Noor wants to make an oat milk cappuccino for the customer.\"\n",
      "      ],\n",
      "      \"timeline\": [\n",
      "        \"<non-serializable: Event>\",\n",
      "        \"<non-serializable: Event>\"\n",
      "      ],\n",
      "      \"bridging_rules\": [\n",
      "        \"If a coworker's action contradicts what was seen, then the belief should align with what's currently observed.\",\n",
      "        \"A belief that aligns with current observation trumps previous knowledge.\"\n",
      "      ],\n",
      "      \"answer\": \"(a) Noor believes the milk pitcher contains almond milk.\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "[[ ## program_outputs ## ]]\n",
      "{\n",
      "  \"world_facts\": [\n",
      "    \"Noor is working at a busy coffee shop.\",\n",
      "    \"Noor wants to make an oat milk cappuccino for the customer.\"\n",
      "  ],\n",
      "  \"timeline\": [\n",
      "    \"<non-serializable: Event>\",\n",
      "    \"<non-serializable: Event>\"\n",
      "  ],\n",
      "  \"bridging_rules\": [\n",
      "    \"If a coworker's action contradicts what was seen, then the belief should align with what's currently observed.\",\n",
      "    \"A belief that aligns with current observation trumps previous knowledge.\"\n",
      "  ],\n",
      "  \"answer\": \"(a) Noor believes the milk pitcher contains almond milk.\"\n",
      "}\n",
      "\n",
      "[[ ## reward_code ## ]]\n",
      "        def reward_fn(args, pred) -> float:\n",
      "            score = 0.0\n",
      "            # 1) final answer present\n",
      "            if getattr(pred, \"answer\", \"\").strip():\n",
      "                score += 1\n",
      "\n",
      "            return score\n",
      "\n",
      "\n",
      "[[ ## target_threshold ## ]]\n",
      "2.5\n",
      "\n",
      "[[ ## reward_value ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## module_names ## ]]\n",
      "[\n",
      "  \"self\"\n",
      "]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## discussion ## ]]`, then `[[ ## advice ## ]]` (must be formatted as a valid Python dict[str, str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m```json\n",
      "{\n",
      "  \"[[## discussion ##]]\": \"The 'self' module correctly identified that Noor believes the milk pitcher contains almond milk based on her observation of her coworker swapping the oat milk. However, this belief contradicts Noor's original intention to prepare an oat milk cappuccino for a customer who specifically asked for it.\",\n",
      "  \n",
      "  \"[[## advice ##]]\": {\n",
      "    \"self\": (\n",
      "      \"In similar situations where you observe someone performing actions that conflict with your initial understanding or intentions based on the customer's request (e.g., swapping ingredients), Noor should trust her immediate observation over previous knowledge. This scenario highlights a need to double-check crucial tasks like fulfilling customer orders.\"\n",
      "      \n",
      "    )\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"[[## completed ##]]\": \"Completed\"\n",
      "}\n",
      "```\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:11:30.009483]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `program_code` (str): The code of the program that we are analyzing\n",
      "2. `modules_defn` (str): The definition of each module in the program, including its I/O\n",
      "3. `program_inputs` (str): The inputs to the program that we are analyzing\n",
      "4. `program_trajectory` (str): The trajectory of the program's execution, showing each module's I/O\n",
      "5. `program_outputs` (str): The outputs of the program that we are analyzing\n",
      "6. `reward_code` (str): The code of the reward function that we are analyzing\n",
      "7. `target_threshold` (float): The target threshold for the reward function\n",
      "8. `reward_value` (float): The reward value assigned to the program's outputs\n",
      "9. `module_names` (list[str]): The names of the modules in the program, for which we seek advice\n",
      "Your output fields are:\n",
      "1. `discussion` (str): Discussing blame of where each module went wrong, if it did\n",
      "2. `advice` (dict[str, str]): For each module, describe very concretely, in this order: the specific scenarios in which it has made mistakes in the past and what each mistake was, followed by what it should do differently in that kind ofscenario in the future. If the module is not to blame, write N/A.\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "Inputs will have the following structure:\n",
      "\n",
      "[[ ## program_code ## ]]\n",
      "{program_code}\n",
      "\n",
      "[[ ## modules_defn ## ]]\n",
      "{modules_defn}\n",
      "\n",
      "[[ ## program_inputs ## ]]\n",
      "{program_inputs}\n",
      "\n",
      "[[ ## program_trajectory ## ]]\n",
      "{program_trajectory}\n",
      "\n",
      "[[ ## program_outputs ## ]]\n",
      "{program_outputs}\n",
      "\n",
      "[[ ## reward_code ## ]]\n",
      "{reward_code}\n",
      "\n",
      "[[ ## target_threshold ## ]]\n",
      "{target_threshold}\n",
      "\n",
      "[[ ## reward_value ## ]]\n",
      "{reward_value}\n",
      "\n",
      "[[ ## module_names ## ]]\n",
      "{module_names}\n",
      "\n",
      "Outputs will be a JSON object with the following fields.\n",
      "\n",
      "{\n",
      "  \"discussion\": \"{discussion}\",\n",
      "  \"advice\": \"{advice}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"object\\\", \\\"additionalProperties\\\": {\\\"type\\\": \\\"string\\\"}}\"\n",
      "}\n",
      "In adhering to this structure, your objective is: \n",
      "        In the discussion, assign blame to each module that contributed to the final reward being below the threshold, if\n",
      "        any. Then, prescribe concrete advice of how the module should act on its future input when we retry the process, if\n",
      "        it were to receive the same or similar inputs. If a module is not to blame, the advice should be N/A.\n",
      "        The module will not see its own history, so it needs to rely on entirely concrete and actionable advice from you\n",
      "        to avoid the same mistake on the same or similar inputs.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## program_code ## ]]\n",
      "class Predict(Module, Parameter):\n",
      "    def __init__(self, signature: str | type[Signature], callbacks: list[BaseCallback] | None = None, **config):\n",
      "        super().__init__(callbacks=callbacks)\n",
      "        self.stage = random.randbytes(8).hex()\n",
      "        self.signature = ensure_signature(signature)\n",
      "        self.config = config\n",
      "        self.reset()\n",
      "\n",
      "    def reset(self):\n",
      "        self.lm = None\n",
      "        self.traces = []\n",
      "        self.train = []\n",
      "        self.demos = []\n",
      "\n",
      "    def dump_state(self):\n",
      "        state_keys = [\"traces\", \"train\"]\n",
      "        state = {k: getattr(self, k) for k in state_keys}\n",
      "\n",
      "        state[\"demos\"] = []\n",
      "        for demo in self.demos:\n",
      "            demo = demo.copy()\n",
      "\n",
      "            for field in demo:\n",
      "                # FIXME: Saving BaseModels as strings in examples doesn't matter because you never re-access as an object\n",
      "                demo[field] = serialize_object(demo[field])\n",
      "\n",
      "            state[\"demos\"].append(demo)\n",
      "\n",
      "        state[\"signature\"] = self.signature.dump_state()\n",
      "        state[\"lm\"] = self.lm.dump_state() if self.lm else None\n",
      "        return state\n",
      "\n",
      "    def load_state(self, state: dict) -> \"Predict\":\n",
      "        \"\"\"Load the saved state of a `Predict` object.\n",
      "\n",
      "        Args:\n",
      "            state: The saved state of a `Predict` object.\n",
      "\n",
      "        Returns:\n",
      "            Self to allow method chaining.\n",
      "        \"\"\"\n",
      "        excluded_keys = [\"signature\", \"extended_signature\", \"lm\"]\n",
      "        for name, value in state.items():\n",
      "            # `excluded_keys` are fields that go through special handling.\n",
      "            if name not in excluded_keys:\n",
      "                setattr(self, name, value)\n",
      "\n",
      "        self.signature = self.signature.load_state(state[\"signature\"])\n",
      "        self.lm = LM(**state[\"lm\"]) if state[\"lm\"] else None\n",
      "\n",
      "        if \"extended_signature\" in state:  # legacy, up to and including 2.5, for CoT.\n",
      "            raise NotImplementedError(\"Loading extended_signature is no longer supported in DSPy 2.6+\")\n",
      "\n",
      "        return self\n",
      "\n",
      "    def _get_positional_args_error_message(self):\n",
      "        input_fields = list(self.signature.input_fields.keys())\n",
      "        return (\n",
      "            \"Positional arguments are not allowed when calling `dspy.Predict`, must use keyword arguments \"\n",
      "            f\"that match your signature input fields: '{', '.join(input_fields)}'. For example: \"\n",
      "            f\"`predict({input_fields[0]}=input_value, ...)`.\"\n",
      "        )\n",
      "\n",
      "    def __call__(self, *args, **kwargs):\n",
      "        if args:\n",
      "            raise ValueError(self._get_positional_args_error_message())\n",
      "\n",
      "        return super().__call__(**kwargs)\n",
      "\n",
      "    async def acall(self, *args, **kwargs):\n",
      "        if args:\n",
      "            raise ValueError(self._get_positional_args_error_message())\n",
      "\n",
      "        return await super().acall(**kwargs)\n",
      "\n",
      "    def _forward_preprocess(self, **kwargs):\n",
      "        # Extract the three privileged keyword arguments.\n",
      "        assert \"new_signature\" not in kwargs, \"new_signature is no longer a valid keyword argument.\"\n",
      "        signature = ensure_signature(kwargs.pop(\"signature\", self.signature))\n",
      "        demos = kwargs.pop(\"demos\", self.demos)\n",
      "        config = dict(**self.config, **kwargs.pop(\"config\", {}))\n",
      "\n",
      "        # Get the right LM to use.\n",
      "        lm = kwargs.pop(\"lm\", self.lm) or settings.lm\n",
      "\n",
      "        if lm is None:\n",
      "            raise ValueError(\n",
      "                \"No LM is loaded. Please configure the LM using `dspy.configure(lm=dspy.LM(...))`. e.g, \"\n",
      "                \"`dspy.configure(lm=dspy.LM('openai/gpt-4o-mini'))`\"\n",
      "            )\n",
      "\n",
      "        if isinstance(lm, str):\n",
      "            # Many users mistakenly use `dspy.configure(lm=\"openai/gpt-4o-mini\")` instead of\n",
      "            # `dspy.configure(lm=dspy.LM(\"openai/gpt-4o-mini\"))`, so we are providing a specific error message.\n",
      "            raise ValueError(\n",
      "                f\"LM must be an instance of `dspy.BaseLM`, not a string. Instead of using a string like \"\n",
      "                f\"'dspy.configure(lm=\\\"{lm}\\\")', please configure the LM like 'dspy.configure(lm=dspy.LM(\\\"{lm}\\\"))'\"\n",
      "            )\n",
      "        elif not isinstance(lm, BaseLM):\n",
      "            raise ValueError(f\"LM must be an instance of `dspy.BaseLM`, not {type(lm)}. Received `lm={lm}`.\")\n",
      "\n",
      "        # If temperature is unset or <=0.15, and n > 1, set temperature to 0.7 to keep randomness.\n",
      "        temperature = config.get(\"temperature\") or lm.kwargs.get(\"temperature\")\n",
      "        num_generations = config.get(\"n\") or lm.kwargs.get(\"n\") or lm.kwargs.get(\"num_generations\") or 1\n",
      "\n",
      "        if (temperature is None or temperature <= 0.15) and num_generations > 1:\n",
      "            config[\"temperature\"] = 0.7\n",
      "\n",
      "        if \"prediction\" in kwargs:\n",
      "            if (\n",
      "                isinstance(kwargs[\"prediction\"], dict)\n",
      "                and kwargs[\"prediction\"].get(\"type\") == \"content\"\n",
      "                and \"content\" in kwargs[\"prediction\"]\n",
      "            ):\n",
      "                # If the `prediction` is the standard predicted outputs format\n",
      "                # (https://platform.openai.com/docs/guides/predicted-outputs), we remove it from input kwargs and add it\n",
      "                # to the lm kwargs.\n",
      "                config[\"prediction\"] = kwargs.pop(\"prediction\")\n",
      "\n",
      "        if not all(k in kwargs for k in signature.input_fields):\n",
      "            present = [k for k in signature.input_fields if k in kwargs]\n",
      "            missing = [k for k in signature.input_fields if k not in kwargs]\n",
      "            logger.warning(\n",
      "                \"Not all input fields were provided to module. Present: %s. Missing: %s.\",\n",
      "                present,\n",
      "                missing,\n",
      "            )\n",
      "        return lm, config, signature, demos, kwargs\n",
      "\n",
      "    def _forward_postprocess(self, completions, signature, **kwargs):\n",
      "        pred = Prediction.from_completions(completions, signature=signature)\n",
      "        if kwargs.pop(\"_trace\", True) and settings.trace is not None and settings.max_trace_size > 0:\n",
      "            trace = settings.trace\n",
      "            if len(trace) >= settings.max_trace_size:\n",
      "                trace.pop(0)\n",
      "            trace.append((self, {**kwargs}, pred))\n",
      "        return pred\n",
      "\n",
      "    def _should_stream(self):\n",
      "        stream_listeners = settings.stream_listeners or []\n",
      "        should_stream = settings.send_stream is not None\n",
      "        if should_stream and len(stream_listeners) > 0:\n",
      "            should_stream = any(stream_listener.predict == self for stream_listener in stream_listeners)\n",
      "\n",
      "        return should_stream\n",
      "\n",
      "    def forward(self, **kwargs):\n",
      "        lm, config, signature, demos, kwargs = self._forward_preprocess(**kwargs)\n",
      "\n",
      "        adapter = settings.adapter or ChatAdapter()\n",
      "\n",
      "        if self._should_stream():\n",
      "            with settings.context(caller_predict=self):\n",
      "                completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "        else:\n",
      "            with settings.context(send_stream=None):\n",
      "                completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "\n",
      "        return self._forward_postprocess(completions, signature, **kwargs)\n",
      "\n",
      "    async def aforward(self, **kwargs):\n",
      "        lm, config, signature, demos, kwargs = self._forward_preprocess(**kwargs)\n",
      "\n",
      "        adapter = settings.adapter or ChatAdapter()\n",
      "        if self._should_stream():\n",
      "            with settings.context(caller_predict=self):\n",
      "                completions = await adapter.acall(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "        else:\n",
      "            with settings.context(send_stream=None):\n",
      "                completions = await adapter.acall(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)\n",
      "\n",
      "        return self._forward_postprocess(completions, signature, **kwargs)\n",
      "\n",
      "    def update_config(self, **kwargs):\n",
      "        self.config = {**self.config, **kwargs}\n",
      "\n",
      "    def get_config(self):\n",
      "        return self.config\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}({self.signature})\"\n",
      "\n",
      "\n",
      "[[ ## modules_defn ## ]]\n",
      "--------------------------------------------------------------------------------\n",
      "Module self\n",
      "\tInput Fields:\n",
      "\t\t1. `story` (str): Short story/problem statement\n",
      "\t\t2. `choices` (list[str]):\n",
      "\tOutput Fields:\n",
      "\t\t1. `world_facts` (list[str]): \n",
      "\t\t2. `timeline` (list[Event]): \n",
      "\t\t3. `bridging_rules` (list[str]): \n",
      "\t\t4. `answer` (str):\n",
      "\tOriginal Instructions: \n",
      "\t\tExtract implicit reasoning needed for a ToM question, then answer.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[[ ## program_inputs ## ]]\n",
      "{\n",
      "  \"story\": \"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\n\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\",\n",
      "  \"choices\": [\n",
      "    \"(a) Noor believes the milk pitcher contains almond milk.\",\n",
      "    \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "[[ ## program_trajectory ## ]]\n",
      "[\n",
      "  {\n",
      "    \"module_name\": \"self\",\n",
      "    \"inputs\": {\n",
      "      \"story\": \"Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\\n\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\",\n",
      "      \"choices\": [\n",
      "        \"(a) Noor believes the milk pitcher contains almond milk.\",\n",
      "        \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
      "      ],\n",
      "      \"hint_\": \"(a) Noor should trust what she sees in front of her, which would lead to believing that the pitcher contains almond milk. To avoid confusion like this happening again, Noor could consider implementing double-checking mechanisms for crucial tasks such as fulfilling customer orders.\"\n",
      "    },\n",
      "    \"outputs\": {\n",
      "      \"world_facts\": [\n",
      "        \"Noor is working at a busy coffee shop.\",\n",
      "        \"Noor wants to make an oat milk cappuccino for the customer.\"\n",
      "      ],\n",
      "      \"timeline\": [\n",
      "        \"<non-serializable: Event>\",\n",
      "        \"<non-serializable: Event>\"\n",
      "      ],\n",
      "      \"bridging_rules\": [\n",
      "        \"If a coworker's action contradicts what was seen, then the belief should align with what's currently observed.\",\n",
      "        \"A belief that aligns with current observation trumps previous knowledge.\"\n",
      "      ],\n",
      "      \"answer\": \"(a) Noor believes the milk pitcher contains almond milk.\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "[[ ## program_outputs ## ]]\n",
      "{\n",
      "  \"world_facts\": [\n",
      "    \"Noor is working at a busy coffee shop.\",\n",
      "    \"Noor wants to make an oat milk cappuccino for the customer.\"\n",
      "  ],\n",
      "  \"timeline\": [\n",
      "    \"<non-serializable: Event>\",\n",
      "    \"<non-serializable: Event>\"\n",
      "  ],\n",
      "  \"bridging_rules\": [\n",
      "    \"If a coworker's action contradicts what was seen, then the belief should align with what's currently observed.\",\n",
      "    \"A belief that aligns with current observation trumps previous knowledge.\"\n",
      "  ],\n",
      "  \"answer\": \"(a) Noor believes the milk pitcher contains almond milk.\"\n",
      "}\n",
      "\n",
      "[[ ## reward_code ## ]]\n",
      "        def reward_fn(args, pred) -> float:\n",
      "            score = 0.0\n",
      "            # 1) final answer present\n",
      "            if getattr(pred, \"answer\", \"\").strip():\n",
      "                score += 1\n",
      "\n",
      "            return score\n",
      "\n",
      "\n",
      "[[ ## target_threshold ## ]]\n",
      "2.5\n",
      "\n",
      "[[ ## reward_value ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## module_names ## ]]\n",
      "[\n",
      "  \"self\"\n",
      "]\n",
      "\n",
      "Respond with a JSON object in the following order of fields: `discussion`, then `advice` (must be formatted as a valid Python dict[str, str]).\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m{\n",
      "  \"discussion\": \"The 'self' module correctly identified that Noor believes the milk pitcher contains almond milk based on what she sees her coworker doing. The bridging rules applied here are reasonable; if there's an observable contradiction between previous knowledge and current observation (such as seeing a swap happen), then aligning with what's currently observed should take precedence over prior beliefs or instructions.\\n\\nHowever, this situation highlights the potential for confusion in Noor's reasoning process when it comes to fulfilling customer orders. In real-life scenarios like these, implementing some form of verification mechanism could help avoid mistakes and improve confidence in task completion.\",\n",
      "  \"advice\": {\n",
      "    \"self\": \"(a) Noor should trust what she sees directly unless there is additional context or evidence that suggests otherwise.\\n\\nFor future tasks involving fulfilling customer orders:\\n- Consider adding a double-checking step where you confirm the ingredients with another coworker before serving, especially for high-priority customizations like those requested by customers.\"\n",
      "  }\n",
      "}\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:11:33.460511]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `story` (str): Short story/problem statement\n",
      "2. `choices` (list[str]): \n",
      "3. `hint_` (str): A hint to the module from an earlier run\n",
      "Your output fields are:\n",
      "1. `world_facts` (list[str]): \n",
      "2. `timeline` (list[Event]): \n",
      "3. `bridging_rules` (list[str]): \n",
      "4. `answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## story ## ]]\n",
      "{story}\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "{choices}\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "{hint_}\n",
      "\n",
      "[[ ## world_facts ## ]]\n",
      "{world_facts}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## timeline ## ]]\n",
      "{timeline}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"$defs\": {\"Event\": {\"type\": \"object\", \"properties\": {\"action\": {\"type\": \"string\", \"title\": \"Action\"}, \"actor\": {\"type\": \"string\", \"title\": \"Actor\"}, \"belief\": {\"type\": \"string\", \"title\": \"Belief\"}, \"location\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"default\": null, \"title\": \"Location\"}, \"order\": {\"type\": \"integer\", \"title\": \"Order\"}}, \"required\": [\"order\", \"actor\", \"action\", \"belief\"], \"title\": \"Event\"}}, \"items\": {\"$ref\": \"#/$defs/Event\"}}\n",
      "\n",
      "[[ ## bridging_rules ## ]]\n",
      "{bridging_rules}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract implicit reasoning needed for a ToM question, then answer.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## story ## ]]\n",
      "Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "[\"(a) Noor believes the milk pitcher contains almond milk.\", \"(b) Noor believes the milk pitcher contains oat milk.\"]\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "(a) Noor should trust what she sees directly unless there is additional context or evidence that suggests otherwise.\n",
      "\n",
      "For future tasks involving fulfilling customer orders:\n",
      "- Consider adding a double-checking step where you confirm the ingredients with another coworker before serving, especially for high-priority customizations like those requested by customers.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## world_facts ## ]]` (must be formatted as a valid Python list[str]), then `[[ ## timeline ## ]]` (must be formatted as a valid Python list[Event]), then `[[ ## bridging_rules ## ]]` (must be formatted as a valid Python list[str]), then `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m```json\n",
      "[\n",
      "  \"## world_facts ##\n",
      "  [\n",
      "    'Noor is working at a busy coffee shop.',\n",
      "    'A customer asked Noor to make cappuccino using oat milk.'\n",
      "  ]\n",
      "  \n",
      "  [[ ## timeline ## ]]\n",
      "  [\n",
      "    {\n",
      "      \"action\": \"grabs\",\n",
      "      \"actor\": \"Noor\",\n",
      "      \"belief\": null,\n",
      "      \"location\": \"'milk pitcher'\",\n",
      "      \"order\": 1\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"fills with\",\n",
      "      \"actor\": \"Coworker\", // Although Noor didn't explicitly fill the milk, this is implied.\n",
      "      \"believe\": \"almond milk instead of oat milk.\",\n",
      "      \"location\": null,\n",
      "      \"order\": 2\n",
      "    }\n",
      "  ]\n",
      "  \n",
      "  [[ ## bridging_rules ##\n",
      "  [\n",
      "    'If an actor performs a task that results in changing another item’s content without direct confirmation from the original requester, then Noor is likely to observe this change and form beliefs based on her observations.'\n",
      "  ]\n",
      "\n",
      "  [[ ## answer ##\n",
      "  \"a) Noor believes the milk pitcher contains almond milk.\"\n",
      "  \n",
      "  ]]\n",
      "\n",
      "  [## completed ##\n",
      "]]\n",
      "```\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001B[34m[2025-08-28T13:11:38.644988]\u001B[0m\n",
      "\n",
      "\u001B[31mSystem message:\u001B[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `story` (str): Short story/problem statement\n",
      "2. `choices` (list[str]): \n",
      "3. `hint_` (str): A hint to the module from an earlier run\n",
      "Your output fields are:\n",
      "1. `world_facts` (list[str]): \n",
      "2. `timeline` (list[Event]): \n",
      "3. `bridging_rules` (list[str]): \n",
      "4. `answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "Inputs will have the following structure:\n",
      "\n",
      "[[ ## story ## ]]\n",
      "{story}\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "{choices}\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "{hint_}\n",
      "\n",
      "Outputs will be a JSON object with the following fields.\n",
      "\n",
      "{\n",
      "  \"world_facts\": \"{world_facts}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"timeline\": \"{timeline}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"$defs\\\": {\\\"Event\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"action\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Action\\\"}, \\\"actor\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Actor\\\"}, \\\"belief\\\": {\\\"type\\\": \\\"string\\\", \\\"title\\\": \\\"Belief\\\"}, \\\"location\\\": {\\\"anyOf\\\": [{\\\"type\\\": \\\"string\\\"}, {\\\"type\\\": \\\"null\\\"}], \\\"default\\\": null, \\\"title\\\": \\\"Location\\\"}, \\\"order\\\": {\\\"type\\\": \\\"integer\\\", \\\"title\\\": \\\"Order\\\"}}, \\\"required\\\": [\\\"order\\\", \\\"actor\\\", \\\"action\\\", \\\"belief\\\"], \\\"title\\\": \\\"Event\\\"}}, \\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Event\\\"}}\",\n",
      "  \"bridging_rules\": \"{bridging_rules}        # note: the value you produce must adhere to the JSON schema: {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}\",\n",
      "  \"answer\": \"{answer}\"\n",
      "}\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract implicit reasoning needed for a ToM question, then answer.\n",
      "\n",
      "\n",
      "\u001B[31mUser message:\u001B[0m\n",
      "\n",
      "[[ ## story ## ]]\n",
      "Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor sees her coworker swapping the milk.\n",
      "\n",
      "Question: Does Noor believe the milk pitcher contains oat milk or almond milk?What will Noor do?\n",
      "\n",
      "[[ ## choices ## ]]\n",
      "[\"(a) Noor believes the milk pitcher contains almond milk.\", \"(b) Noor believes the milk pitcher contains oat milk.\"]\n",
      "\n",
      "[[ ## hint_ ## ]]\n",
      "(a) Noor should trust what she sees directly unless there is additional context or evidence that suggests otherwise.\n",
      "\n",
      "For future tasks involving fulfilling customer orders:\n",
      "- Consider adding a double-checking step where you confirm the ingredients with another coworker before serving, especially for high-priority customizations like those requested by customers.\n",
      "\n",
      "Respond with a JSON object in the following order of fields: `world_facts` (must be formatted as a valid Python list[str]), then `timeline` (must be formatted as a valid Python list[Event]), then `bridging_rules` (must be formatted as a valid Python list[str]), then `answer`.\n",
      "\n",
      "\n",
      "\u001B[31mResponse:\u001B[0m\n",
      "\n",
      "\u001B[32m{\n",
      "  \"world_facts\": [\n",
      "    \"Noor is working at a busy coffee shop.\",\n",
      "    \"A customer asked for oat milk in their cappuccino order.\"\n",
      "  ],\n",
      "  \"timeline\": [\n",
      "    {\n",
      "      \"order\": 1,\n",
      "      \"actor\": \"Coworker\",\n",
      "      \"action\": \"Swaps the almond milk with oat milk in the pitcher while Noor is attending to another task\"\n",
      "    ,\n",
      "      \"belief\": \"Assuming the customer didn't specify any preference for nut-free or dairy products, a common belief.\"\n",
      "    , \"location\": null\n",
      "  },\n",
      "  {\n",
      "    \"order\": 2,\n",
      "    \"actor\": \"Noor\",\n",
      "    \"action\": \"Sees her coworker swapping almond milk with oat milk in the pitcher.\",\n",
      "    \"belief\": \"Assuming she hasn't been notified of any change by another colleague or customer.\"\n",
      "  , \"location\": null\n",
      " },\n",
      "   {\n",
      "     \"order\": 3,\n",
      "     \"actor\": \"Noor\",\n",
      "     \"action\": \"Believes what is directly seen unless there are additional contexts indicating otherwise.\",\n",
      "\n",
      "    \"belief\": \"Assuming she hasn't been notified of any change by another colleague or customer.\",\n",
      "\"location\": null\n",
      " }\n",
      "],\n",
      "\"bridging_rules\": [\n",
      " \"If Noor sees her coworker swapping almond milk with oat milk, and assuming no other context was given to indicate a different scenario (like the customer's preference for nut-free options), then it is reasonable to deduce that she believes there are only almonds in the pitcher.\" ,\n",
      "\"The action of seeing directly observed events usually forms the basis of belief unless contradicted by additional information.\"\n",
      "],\n",
      "\"answer\": \"(a) Noor believes the milk pitcher contains almond milk\"\n",
      "}\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:12:31.482971Z",
     "start_time": "2025-08-28T11:11:38.709785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Event(BaseModel):\n",
    "    order: int\n",
    "    actor: str\n",
    "    action: str\n",
    "    belief: str\n",
    "    location: str | None = None\n",
    "\n",
    "class ToMSignature(dspy.Signature):\n",
    "    \"\"\"Extract implicit reasoning needed for a ToM question, then answer.\"\"\"\n",
    "    story: str = dspy.InputField(desc=\"Short story/problem statement\")\n",
    "    choices: list[str] = dspy.InputField()\n",
    "\n",
    "    world_facts: List[str] = dspy.OutputField()\n",
    "    timeline: List[Event] = dspy.OutputField()\n",
    "    bridging_rules: List[str] = dspy.OutputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "    answer_choice: str = dspy.OutputField()\n",
    "\n",
    "\n",
    "class ToMExtractor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = dspy.Predict(ToMSignature)\n",
    "\n",
    "        def reward_fn(args, pred) -> float:\n",
    "            score = 0.0\n",
    "            # 1) final answer present\n",
    "            if getattr(pred, \"answer\", \"\").strip():\n",
    "                score += 1\n",
    "\n",
    "            return score\n",
    "\n",
    "        # Try up to N times; stop when score >= threshold\n",
    "        self.extract = dspy.Refine(\n",
    "            module=base,\n",
    "            N=4,\n",
    "            reward_fn=reward_fn,\n",
    "            threshold=2.5,   # require any 2–3 of the checks to pass\n",
    "        )\n",
    "\n",
    "    def forward(self, story: str, choices: List[str]):\n",
    "        return self.extract(story=story, choices=choices)\n",
    "\n",
    "\n",
    "\n",
    "# 4) Run it on your ToM example\n",
    "story = (\n",
    "  \"Noor is working as a barista at a busy coffee shop. Noor wants to make a \"\n",
    "  \"delicious cappuccino for a customer who asked for oat milk. Noor grabs a \"\n",
    "  \"milk pitcher and fills it with oat milk. \"\n",
    "  \"A coworker, who didn't hear the customer's request, \"\n",
    "  \"swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. \"\n",
    "  \"Noor sees her coworker swapping the milk.\"\n",
    "  \"What will Noor do?\"\n",
    ")\n",
    "choices = [\"(a) Noor believes the milk pitcher contains almond milk.\",\n",
    "           \"(b) Noor believes the milk pitcher contains oat milk.\"\n",
    "]\n",
    "\n",
    "extractor = ToMExtractor()\n",
    "result = extractor(story=story, choices=choices)\n",
    "\n",
    "# 5) Inspect the structured “reasoning steps”\n",
    "print(\"World facts:\", *result.world_facts, sep=\"\\n- \")\n",
    "print(\"\\nTimeline:\", *[e.model_dump() for e in result.timeline], sep=\"\\n- \")\n",
    "print(\"\\nBridging rules:\", *result.bridging_rules, sep=\"\\n- \")\n",
    "print(\"\\nFinal answer:\", result.answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World facts:\n",
      "- Noor is working at a coffee shop.\n",
      "- A customer asked for oat milk in their cappuccino order.\n",
      "- Noor grabbed the pitcher and filled it with oat milk to make this cappuccino.\n",
      "- The coworker swapped almond milk instead of oat milk while Noor was attending another task.\n",
      "\n",
      "Timeline:\n",
      "- {'order': 1, 'actor': 'Noor', 'action': \"filling a milk pitcher with oat milk for customer's order\", 'belief': 'the customer requested an oat milk cappuccino.', 'location': None}\n",
      "- {'order': 2, 'actor': 'coworker', 'action': 'swapping almond milk instead of the filled oat milk in the pitcher.', 'belief': \"Noor didn't specify that she needs to use only almond or not using any other kind of milk.\", 'location': None}\n",
      "\n",
      "Bridging rules:\n",
      "- If Noor saw her coworker swapping a different type of milk into what was supposed to be an empty pitcher, then it affects the belief about what's inside the pitcher at this moment.\n",
      "\n",
      "Final answer: Noor believes that there is almond milk in the pitcher.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:12:31.501311Z",
     "start_time": "2025-08-28T11:12:31.496958Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"\\nFinal choice:\", result.answer_choice)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final choice: (a) Noor believes the milk pitcher contains almond milk.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:12:31.513518Z",
     "start_time": "2025-08-28T11:12:31.510495Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
